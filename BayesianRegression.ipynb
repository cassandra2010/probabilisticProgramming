{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate toy dataset\n",
    "#one feature\n",
    "#w=3\n",
    "#b=1\n",
    "\n",
    "N = 100 #size of toy data\n",
    "def build_linear_dataset(N , p=1,noise_std =0.01):\n",
    "    X = np.random.rand(N,p)\n",
    "    w = 3*np.ones(p)\n",
    "    #np.repeat(1,N) basically adds b =1 \n",
    "    y = np.matmul(X,w) + np.repeat(1,N) + np.random.normal(0,noise_std,size=N)\n",
    "    y=y.reshape(N,1)\n",
    "    X,y = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "    data = torch.cat((X,y),1)\n",
    "    assert data.shape == (N,p+1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self ,p):\n",
    "        #p : number of features\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p,1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "regression_model = RegressionModel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "#mean sq err as loss\n",
    "#adam optimizer\n",
    "#learning rate 0.01\n",
    "#iterations 500\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(regression_model.parameters(),lr=0.5)\n",
    "num_iterations = 1000 if not smoke_test else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss : 0.9076\n",
      "[iteration 0100] loss : 0.0108\n",
      "[iteration 0150] loss : 0.0098\n",
      "[iteration 0200] loss : 0.0097\n",
      "[iteration 0250] loss : 0.0097\n",
      "[iteration 0300] loss : 0.0097\n",
      "[iteration 0350] loss : 0.0097\n",
      "[iteration 0400] loss : 0.0097\n",
      "[iteration 0450] loss : 0.0097\n",
      "[iteration 0500] loss : 0.0097\n",
      "[iteration 0550] loss : 0.0097\n",
      "[iteration 0600] loss : 0.0097\n",
      "[iteration 0650] loss : 0.0097\n",
      "[iteration 0700] loss : 0.0097\n",
      "[iteration 0750] loss : 0.0097\n",
      "[iteration 0800] loss : 0.0097\n",
      "[iteration 0850] loss : 0.0097\n",
      "[iteration 0900] loss : 0.0097\n",
      "[iteration 0950] loss : 0.0097\n",
      "[iteration 1000] loss : 0.0097\n",
      "learned parameters : \n",
      "linear.weight: 2.999\n",
      "linear.bias: 1.001\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = build_linear_dataset(N)\n",
    "    x_data = data[:,:-1]\n",
    "    y_data = data[: , -1]\n",
    "    for j in range(num_iterations):\n",
    "        #run model forward\n",
    "        y_pred = regression_model(x_data).squeeze(-1)\n",
    "        \n",
    "        #calculate mse loss\n",
    "        loss = loss_fn(y_pred,y_data)\n",
    "        \n",
    "        #initialise gradients to 0\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        #backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        #take a gradient step\n",
    "        optim.step()\n",
    "        \n",
    "        if(j+1)%50 == 0:\n",
    "            print(\"[iteration %04d] loss : %.4f\" %(j+1,loss.item()))\n",
    "            \n",
    "            \n",
    "    print(\"learned parameters : \")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: %.3f\" %(name,param.data.numpy()))\n",
    "\n",
    "if __name__== '__main__':\n",
    "    main()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we confident of above estimates ?\n",
    "We need Bayesian modelling to reason about the model uncertainty .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than learning point estimates on w and b , we will learn a distribution over values of w and b consistent with the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make linear regression Bayesian , put priors on w and b .\n",
    "Priors : distributions representing out prior belief about reasonable values for w and b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_module() : takes a given nn.Mosule and turns it into distribution over the same module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example , a distribution over regressors . Each parameter in the original regression model is sampled from the provided prior ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def model(data):\n",
    "    #create unit normal priors over the parameters\n",
    "    loc , scale = torch.zeros(1,1) , 10*torch.ones(1,1)\n",
    "    bias_loc , bias_scale = torch.zeros(1), 10*torch.ones(1)\n",
    "    w_prior = Normal(loc, scale).independent(1)\n",
    "    b_prior = Normal(bias_loc,bias_scale).independent(1)\n",
    "    priors = {'linear.weight' : w_prior , 'linear.bias' : b_prior}\n",
    "    \n",
    "    #lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\" , regression_model,priors)\n",
    "    \n",
    "    #sample a regressor (which also samples w and b) from the prior\n",
    "    lifted_reg_module = lifted_module()\n",
    "    with pyro.iarange(\"map\", N) :\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "        \n",
    "        #run the regressor forward conditioned on data\n",
    "        prediction_mean = lifted_reg_module(x_data).squeeze(-1)\n",
    "        \n",
    "        #condition on the observed data\n",
    "        pyro.sample(\"obs\", Normal(prediction_mean,0.1*torch.ones(data.size(0))),obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do inference weâ€™re going to need a guide : a \n",
    "parameterized family of distributions over w and b. \n",
    "\n",
    "Writing down a guide will proceed in close analogy to the \n",
    "construction of our model, \n",
    "with the key __difference__ that the \n",
    "guide parameters need to be trainable. To do this we register \n",
    "the guide parameters in the ParamStore using pyro.param()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(data):\n",
    "    #variational parameters\n",
    "    w_loc = torch.randn(1,1)\n",
    "    \n",
    "    #initialise scales to be pretty narrow\n",
    "    w_log_sig = torch.tensor(-3.0 * torch.ones(1, 1) + 0.05 * torch.randn(1, 1))\n",
    "    b_loc = torch.randn(1)\n",
    "    b_log_sig = torch.tensor(-3.0 * torch.ones(1) + 0.05 * torch.randn(1))\n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_loc)\n",
    "    #we use softplus to ensure positivity\n",
    "    sw_param = softplus(pyro.param(\"guide_log_scale_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_loc)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_scale_bias\", b_log_sig))\n",
    "    # guide distributions for w and b\n",
    "    w_dist = Normal(mw_param, sw_param).independent(1)\n",
    "    b_dist = Normal(mb_param, sb_param).independent(1)\n",
    "    dists = {'linear.weight': w_dist, 'linear.bias': b_dist}\n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "#use Stochastic Variational Inference (SVI)\n",
    "#instead of MSE , use ELBO objective\n",
    "\n",
    "optim = Adam({\"lr\" : 0.05})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 704.9314\n",
      "[iteration 0101] loss: 3.7550\n",
      "[iteration 0201] loss: 0.2334\n",
      "[iteration 0301] loss: -0.6487\n",
      "[iteration 0401] loss: -1.1334\n",
      "[iteration 0501] loss: -1.0492\n",
      "[iteration 0601] loss: -1.2557\n",
      "[iteration 0701] loss: -1.2460\n",
      "[iteration 0801] loss: -1.2597\n",
      "[iteration 0901] loss: -1.1881\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    data = build_linear_dataset(N)\n",
    "    for j in range(num_iterations):\n",
    "        #calculate loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if j%100 == 0 :\n",
    "            print(\"[iteration %04d] loss: %.4f\" %(j+1, loss/float(N)))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guide_log_scale_bias]: -3.886\n",
      "[guide_log_scale_weight]: -3.680\n",
      "[guide_mean_weight]: 3.001\n",
      "[guide_mean_bias]: 0.989\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"[%s]: %.3f\" %(name, pyro.param(name).data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model by checking its \n",
    "predictive accuracy on new test data : point evaluation. \n",
    "Sample 20 neural nets from our \n",
    "posterior, run them on the test data, then average across \n",
    "their predictions and calculate the MSE of the predicted \n",
    "values compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss: ', 5.852320828125812e-05)\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(6, 7, num=20)\n",
    "y = 3 * X + 1\n",
    "X, y = X.reshape((20, 1)), y.reshape((20, 1))\n",
    "x_data, y_data = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "loss = nn.MSELoss()\n",
    "y_preds = torch.zeros(20, 1)\n",
    "for i in range(20):\n",
    "    # guide does not require the data\n",
    "    sampled_reg_model = guide(None)\n",
    "    # run the regression model and add prediction to total\n",
    "    y_preds = y_preds + sampled_reg_model(x_data)\n",
    "# take the average of the predictions\n",
    "y_preds = y_preds / 20\n",
    "print(\"Loss: \", loss(y_preds, y_data).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
